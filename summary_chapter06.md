# Ch.6 카프카 내부 메커니즘

# 6.1 클러스터 멤버십

- ZooKeeper 대신 **KRaft(= Kafka Raft)** 프로토콜이 클러스터 메타데이터를 관리
- 클러스터에는 **컨트롤러 쿼럼(Controller Quorum)** 이라는 별도의 노드 그룹이 있고, 이들이 메타데이터 로그(metadata log)를 관리
    - 각 브로커는 고유한 `node.id`를 가짐 (예전 `broker.id`와 같은 개념)
    - 브로커가 클러스터에 합류할 때, 메타데이터 쿼럼에 `RegisterBrokerRecord` 이벤트를 기록하여 "나는 살아있다"는 정보를 등록

→ 즉, ZooKeeper의 **ephemeral node** 대신 **메타데이터 로그 이벤트**로 브로커의 참여와 종료가 기록

# 6.2 컨트롤러

## 6.2.1 KRaft: 카프카의 새로운 래프트 기반 컨트롤러

### 1. Zookeeper에서 Raft 기반 컨트롤러로 교체한 이유

1. 브로커, 컨트롤러, 주키퍼 간에 **메타데이터 불일치 발생 가능**
    - 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적
    - 브로커 메시지를 보내는 작업은 비동기적
    - 주키퍼로부터 업데이트를 받는 과정 비동적
2. **병목**
    - 컨트롤러가 재시작될 때마다 주키퍼로부터 모든 브로커와 파티션에 대한 메타데이터를 읽어와서 이 메타데이터를 모든 브로커로 전송
    
    → 파티션과 브로커의 수가 증가하면 병목 심화
    
3. **메타데이터 소유권 관련된 내부 아키텍처 개선 필요**
    - 작업이 컨트롤러, 브로커, 주키퍼로 나뉘어져 있음
4. **높은 러닝커브**
    - 주키퍼 자체로 분산 시스템이기 때문에 어느 정도 기반 지식 필요
    
    → 카프카를 사용하려면 카프카와 주키퍼 두 개의 분산 시스템에 대해 배워야함
    

### 2. KRaft의 핵심 아이디어

카프카 **그 자체**에 사용자가 상태를 이벤트 스트림으로 나타낼 수 있도록 하는 **로그 기반 아키텍처**를 도입한다는 점

- 래프트 알고리즘 사용
    - 컨트롤러 노드들은 외부 시스템에 의존하지 않고 **자체적으로 리더 선출**
- 로그 기반 아키텍처 도입
    - 이벤트 사이에 명확한 순서 부여, 컨슈머들이 항상 하나의 타임라인을 따라 움직이도록 보장
    - 동일한 장점이 카프카 메타데이터를 관리하는데도 적용

### 3. KRaft 모드의 새로운 구조

- 컨트롤러 쿼럼: 여러 개의 컨트롤러 노드가 Raft 그룹을 이루어 메타데이터 로그를 관리
    - **Active Controller(리더)**: 모든 메타데이터 변경 처리
    - **Follower Controller**: 로그 복제 및 **핫 스탠바이
    
    **핫 스탠바이: 현재 활성 컨트롤러는 하나뿐이지만, 다른 컨트롤러 노드들이 항상 최신 상태를 동기화하면서 대기하는 것
    
- **메타데이터 로그**: 토픽, 파티션, ISR, 설정 등 모든 변경 내역이 **순서대로 기록**
- MetadataFetch API: 브로커는 컨트롤러부터 **pull 방식**으로 최신 메타데이터를 가져오고, **디스크에 저장하여 빠른 재시작 가능**
- 브로커 등록: 브로커는 컨트롤러 쿼럼에 등록되며, 오프라인이 되어도 **등록된 상태**로 남음
    - 오래된 메타데이터를 가진 브로커는 **데이터 불일치나 손상 방지**를 위해 **fenced 상태로 차단되어 클라이언트 요청을 처리하지 못함
    
    **fenced: 브로커가 클러스터에 등록은 되어 있지만 **최신 메타데이터를 가지고 있지 않거나 리더 자격을 상실한 경우** **컨트롤러에 의해 차단(fence)** 당하는 상태
    

### 4. KRaft 모드 사용법

1. **클러스터 ID 생성 및 저장 공간 포맷**

```
# 클러스터ID 생성
$ bin/kafka-storage.sh random-uuid

# 저장 공간 포맷
$ bin/kafka-storage.sh format -t {클러스터ID} -c {카프카 설정파일}
```

- 클러스터 ID는 새 클러스터를 설정할 때 **한 번만 생성**
- 포맷 작업은 **서버를 설정할 때마다**

1. **디렉토리**

`cofig/kraft` 디렉토리:

- broker.properties: 브로커 역할 설정
- controller.properties: 컨트롤러 역할 설정
- server.properties: 브로커, 컨트롤러 역할 설정 → **프로덕션에서 사용 비권장**

1. **설정 변경**
- `process.roles`: 해당 인스턴스가 수행하는 역할 설정
    - controller
    - broker
    - broker, controller
- `node.id`: 인스턴스의 id
    - 같은 클러스터에 속한 인스턴스들은 node.id가 모두 달라야 함
        - node.id값이 1인 컨트롤러가 잇다면 node.id 값이 1인 브로커는 있을 수 없음
- `controller.quorum.voters`: 사용할 컨트롤러 쿼럼을 지정
    - 기본 포트: **9093**
    - **{컨트롤러의 node.id값}@{컨트롤러의 호스트명}:{포트명}**
    - 전체 컨트롤러 쿼럼을 모두 지정할 필요는 없지만 2개 이상 지정 권장
- `listeners`: 브로커 리스너와 컨트롤러 리스너도 설정 가능
    - 브로커 역할만 하는 경우: **listeners=PLAINTEXT://:9092**
    - 컨트롤러 역할만 하는 경우: **listeners=CONTROLLER://:9093**
    - 브로커와 컨트롤러 역할 하는 경우: **listeners=PLAINTEXT://:9092,CONTROLLER://:9093**
    - 컨트롤러 리스터 이름은 `controller.listener.names` 설정을 사용해서 변경 가능
- `log.dirs`: 레코드 로그 파일과 메타데이터 로그 파일 저장
    - 브로커 역할: 레코드 로그 파일 저장
    - 컨트롤러 역할: 메타데이터 로그 파일 저장
    - 브로커, 컨트롤러 역할: 레코드, 메타데이터 로그 파일 저장

1. **시작하기**

```
# controller, broker 역할 모두하는 프로세스 시작
$ bin/kafka-server-start.sh config/kraft/server.properties

# controller 프로세스 시작
$ bin/kafka-server-start.sh config/kraft/controller.properties

# broker 프로세스 시작
$ bin/kafka-server-start.sh config/kraft/broker.properties

# 토픽 생성
$ bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092
```

# 6.3 복제

## **복제가 중요한 이유**

개별적인 노드에 필연적으로 장애가 발생할 수 밖에 없는 상황에서 카프카가 **신뢰성과 지속성을 보장하는 방식**

## 레플리카

### 1. 리더 레플리카

각 파티션에서 리더 역할을 하는 하나의 레플리카

- 어느 팔로워가 리더 레플리카의 최신 상태를 유지하고 있는지 확인
- **모든 쓰기 요청은 리더 레플리카가 처리** → 일관성 보장
- 클라이언트들은 **기본적으로 리더 레플리카에서 데이터를 읽음**

### 2. 팔로워 레플리카

파티션에 속한 모든 레플리카 중 리더 레플리카를 제외한 나머지

- 별도로 설정을 잡아주지 않는 한, **클라이언트의 요청 처리 불가(쓰기/읽기)**
- 리더 레플리카로 들어온 최신 메시지 복제, 최신 상태 유지
- 해당 파티션의 리더 레플리카에 크래쉬가 날 경우, 팔로워 레플리카 중 하나가 파티션의 새 리더 파티션으로 승격

### 3. 팔로워로부터 읽기

클라이언트가 리더 레플리카 대신 가장 가까이에 있는 **인-싱크 레플리카**로부터 읽어오록 설정 → 네트워크 트래픽 비용 감소

**사용방법**

- 컨슈머 설정: `client.rack` - 컨슈머의 위치 지정
- 브로커 설정: `replica.selector.class`
    - 기본값: `LeaderSelector` - 항상 리더에서 읽음
    - 변경 가능: `RackAwareReplicaSelector` - 컨슈머와 같은 rack.id를 가진 팔로워 선택
    - 또는 직접 `ReplicaSelector` 인터페이스 구현 가능

**신뢰성 보장**

- 팔로워도 리더의 High Watermark(커밋된 최신 오프셋)를 전달받아 리더에 커밋된 메시지만 읽을 수 있음

→ **리더에서 읽을 때와 동일한 안정성(데이터 일관성) 제공**

**주의할 점**

- 리더에서 읽는 것보다 늦게 읽을 수 있음
    - **High Watermark 전파에 지연**이 있을 수 있어서 리더에서 읽을 때보다 데이터가 늦게 보일 수 있음

**→ 지연을 최소화하려면 리더에서, 네트워크 비용을 줄이고 싶으면 팔로워에서 읽는게 유리**

## 리더와 팔로워의 동기화

### 1. 복제 동작

- 팔로워 레플리카는 리더에게 Fetch 요청(컨슈머와 동일한 요청)을 보내 데이터를 가져옴
- 요청에는 **다음 메시지 오프셋이 포함**, 리더는 팔로워가 어디까지 복제했는지 추적 가능

→ 이를 통해 리더는 각 팔로워의 **동기화 상태 파악** 가능

### 2. ISR(In-Sync Replicas)

- `ISR` : **리더와 동기화 상태를 유지하는 레플리카 집합**
- 팔로워가 10초 이상(기본값) 요청이 없거나, 최신 메시지를 따라잡지 못하면 `Out-of-Sync`로 간주
- ISR에 포함된 레플리카만 새로운 리더가 될 수 있음 → 데이터 일관성 보장
- 허용 지연 시간은 `replica.lag.time.max.ms`로 조정 가능

### 3. 선호 리더(Preferred Leader)

토픽 생성 시 최초 리더였던 레플리카

- 파티션마다 선호 리더가 존재
- **선호 리더를 사용하는 이유:**
    - 파티션 생성 시 리더가 브로커들에 **균등 분포**되도록 지정
    - 선호 리더 = 초기 분산 배치 기준으로 정해진 리더
    - 실제 리더가 모두 선호 리더와 일치 시:
        - 파티션 리더가 브로커 전체에 균**등 분산 → 부하 균형 확보**
- 기본 설정(`auto.leader.rebalance.enable=true`)에서 **선호 리더가 ISR 안에 있을 경우 자동으로 리더로 복귀**

### 4. 관리 포인트

- 현재 ISR과 Preferred Leader는 `kafka-topics.sh` 등 관리 툴로 확인 가능
- 파티션의 **레플리카 리스트에서 첫 번째 레플리카 = 선호 리더**
- 수동으로 레플리카를 재배치 시 첫번째로 지정한 브로커가 선호 리더가 되므로 선호 리더를 서로 다른 브로커들로 분산 필요 → 특정 브로커 과부하 방지

# 6.4 요청 처리

**1. 기본 개념**

- Kafka는 **TCP 기반 바이너리 프로토콜**을 사용해 클라이언트와 통신
- 클라이언트가 항상 연결을 시작하고 요청을 보냄 → 브로커가 처리 후 응답
- 요청은 **도착 순서대로 처리되어 메시지 순서 보장 가능**

**2. 요청 헤더**

- `API Key`: 요청 타입
- `Version`: 클라이언트 버전 호환성
    - 브로커는 서로 다른 버전의 클라이언트로부터 요청을 받아 각각의 버전에 맞는 응답 가능
- `Correlation ID`: 각각의 요청에 붙는 고유한 식별자
    - 요청/응답 매칭 및 트러블 슈팅 용
- `Client ID`: 요청을 보낸 애플리케이션 식별

**3. 요청 처리 흐름**

![image.png](attachment:2df31872-4e58-46ac-9581-d0cf83904c2c:image.png)

- `Acceptor Thread`: 연결 수락 후 Processor Thread에 전달
- `Processor(Network) Thread`: 요청을 큐에 넣고 응답 큐에서 결과를 가져와 전송
- `Request Handler(I/O) Thread`: 요청 처리 담당
- 일부 응답은 **Purgatory에서 대기 - 소비자 fetch, 토픽 삭제 등

**Pugatory: **바로 응답을 보낼 수 없는 요청들을 임시로 보관**하는 큐/대기 공간

- Consumer Fetch 요청:
    
    컨슈머가 데이터를 요청했는데 리더 파티션에 아직 새로운 메시지가 없으면, 브로커는 바로 빈 응답을 주지 않고 "데이터가 도착할 때까지" 요청을 Purgatory에 넣어둡니다. (→ L**ong Polling 방식**)
    
    → 새 메시지가 도착하면 해당 요청을 깨워서 응답을 보냄
    
- **DeleteTopic 요청**
    
    토픽 삭제는 시간이 오래 걸리는 작업이라, 브로커는 요청을 받아두고 삭제가 시작된 뒤 응답을 돌려줍니다. 그동안 요청은 Purgatory에 머무름
    

→ 즉, **조건 충족 시점까지 응답을 지연시키는 메커니즘이며 실시간성을 유지하면서도 불필요한 polling을 줄이는 데 도움**

**4. 요청 유형**

- **쓰기 요청**: Producer 요청 → 메시지 전송
- **읽기 요청**: Consumer/Follower 요청 → 메시지 읽기
- **어드민 요청**: 토픽 생성이나 학제 등 메타데이터 작업을 수행 중인 어드민 클라이언트가 보낸 요청

**5. Metadata 요청**

![image.png](attachment:79ac792e-d6bd-4e2f-a842-628f614e27bc:image.png)

- 클라이언트가 토픽, 파티션, 리더 정보를 얻기 위해 전송
- 모든 브로커에 메타데이터 캐시가 존재 → 어떤 브로커에 보내도 처리 가능
- 클라이언트는 이 정보를 캐시하여 올바른 브로커에 요청을 전달
- 주기적 갱신 필요(`metadata.max.age.ms` 설정)
- **“Not a Leader”** 오류 발생 시 메타데이터 새로고침 후 재시도 필요

## 6.4.1 쓰기 요청

`acks`: 쓰기 작업이 성공한 것으로 간주되기 전, 메시지에 대한 응답을 보내야 하는 브로커의 수

- `acks=1` : **리더만** 메세지를 받음
- `acks=all`: **모든 인-싱크 레플리카들**이 메시지를 받음
- `acks=0`: 메시지가 보내짐(브로커의 **응답을 기다리지 않음**)

**유효성 검증**

파티션의 리더 레플리카를 가지고 있는 브로커가 해당 파티션에 대한 쓰기 요청을 받는 경우:

- 데이터를 보내고 있는 사용자가 토픽에 대한 쓰기 권한을 가지고 있는가?
- 요청에 지정되어 있는 acks 설정값이 올바른가?
- 만약 acks 설정값이 all로 잡혀 있을 경우, 메시지를 안전하게 쓸 수 있을 만큼 충분한 인-싱크 레플리카가 있는가?

**메시지 저장과 acks 처리**

- 유효성 검사 이후 메시지는 리더 파티션 브로커의 로컬 디스크(파일시스템 캐시)에 기록
- Kafka는 디스크 동기화를 기다리지 않고 복제로 내구성 보장
- acks 설정:
    - `0` or `1`: 리더에 기록되면 즉시 응답
    - `all`: 팔로워들이 복제 완료할 때까지 purgatory 대기 후 응답

## 6.4.2 읽기 요청

**읽기 요청 처리**

- **컨슈머 요청**: 특정 토픽/파티션/오프셋 범위, 데이터 상한(limit) 지정 - `fetch.max.bytes`, `max.partition.fetch.bytes`
- 브로커는 요청이 유효한지(해당 오프셋 존재 여부) 확인 후 메시지를 반환
- 메시지 전송 시 ****Zero-copy I/O** 활용 → 파일시스템 캐시 → 네트워크로 바로 전송 (중간 버퍼 없음 → 성능 향상)
- **Lower boundary(데이터 하한) 설정 가능**: “최소 N바이트 모이면 응답해라” → 저빈도 트래픽에서 효율적 - `fetch.min.bytes`
- 하지만 무한 대기는 방지해야 하므로 **timeout** 지정 가능 - `fetch.max.wait.ms`

****Zero-copy I/O:** 일반적인 네트워크로 데이터를 보낼 때의 과정에서 **중간 단계 복사를 제거**하는 기술

- 디스크 → 커널 버퍼 → 유저 공간 버퍼 → 커널 네트워크 버퍼 → 소켓 전송
    - 일반적으로는 디스크에서 읽은 데이터를 여러 번 복사해야 하는데, 이 과정에서 CPU와 메모리 대역폭을 많이 소모
- Kafka는 Linux의 sendfile() 시스템 콜을 이용:
    - 디스크(파일시스템 캐시) → 네트워크 소켓으로 바로 데이터를 전송
    - 유저 공간 버퍼로 데이터를 옮기지 않음
- 장점:
    - CPU 사용량 감소
    - 메모리 복사 오버헤드 제거
    - 고속 데이터 전송 가능 → Kafka가 초당 수백 MB ~ GB 처리가 가능한 핵심 이유 중 하나

**안전한 메시지 읽기**

![image.png](attachment:45458f9d-cd69-46bc-bfcc-1cd6dc6891dd:image.png)

- 컨슈머는 **모든 In-Sync Replica(ISR)에 기록된 메시지만 읽을 수 있음**
- **이유:** 리더만 가지고 있는 메시지를 읽게 하면, 리더 장애 시 **데이터 유실/불일치 발생 가능**
- 따라서 리더는 팔로워에 복제가 완료될 때까지 메시지를 컨슈머에 보내지 않음
- 복제 지연 허용 한도 설정 가능 - `replica.lag.time.max.ms`

## 6.4.3 기타 요청

### 1. 요청 종류

- Kafka 프로토콜은 현재 **61개 이상의 요청 타입**을 지원 (추가 계속됨)
- **Consumer만** 해도 그룹 관리/조정 등에 **15개 요청 타입** 사용
- 메타데이터 관리, 보안 관련 요청도 다수 존재
- **브로커 간 통신**도 동일한 프로토콜 사용 (예: `LeaderAndIsr` 요청으로 리더 교체 알림)

### 2. ZooKeeper 의존성 제거 & 새로운 요청

- 과거:
    - Consumer 오프셋 → ZooKeeper에 저장/조회
    - 토픽 생성 → ZooKeeper 직접 갱신
- 현재:
    - 오프셋 관련 요청 추가:
        - `OffsetCommitRequest`, `OffsetFetchRequest`, `ListOffsetsRequest`
    - 메타데이터 관리 요청 추가:
        - `CreateTopicRequest` 등
    - → 이제는 AdminClient를 통해 브로커에게 직접 요청 가능 (ZooKeeper 불필요)

### 3. 요청 버전 관리

- Kafka는 기존 요청을 확장할 때 **버전(version)** 을 증가시켜 호환성을 유지
- 예: Kafka 0.10.0에서 Metadata Response에 **컨트롤러 정보** 추가 → Metadata 요청 버전 1 생성
    - 0.9.0 클라이언트 = v0 요청/응답만 처리
    - 0.10.0 클라이언트 = v1 요청/응답 처리
- **업그레이드 원칙**: 항상 **브로커 → 클라이언트 순서**로 업그레이드
    - 새 브로커는 옛 요청도 이해 가능, 반대는 불가능

### 4. ApiVersionRequest (0.10.0 추가)

- 클라이언트가 브로커에게 **지원하는 요청 버전 목록**을 질의 가능
- 이를 통해 클라이언트는 브로커가 지원하는 버전을 선택해 통신
- 최근 KIP-584 제안: **클라이언트가 브로커 지원 기능(feature)까지 질의**할 수 있게 확장 예정 (Kafka 3.0.0 목표)

# 6.5 물리적 저장소

## **기본 물리적 저장소**

- Kafka의 기본 저장 단위는 **파티션(replica)**
- 파티션은 **브로커 간 분할 불가**, 한 브로커 내에서도 **여러 디스크에 분할 불가** → 한 파티션 크기는 **단일 디스크/마운트 포인트 크기 제한**
- 관리자는 `log.dirs` 파라미터로 파티션 저장 경로를 지정함 (일반적으로 디스크/마운트 포인트별로 하나씩)

## 6.5.1 계층화된 저장소

### **1. Tiered Storage 등장 배경**

Kafka가 장기간 대용량 데이터를 저장하는 데 쓰이면서 생기는 문제:

- 파티션 크기가 디스크 크기로 제한됨 → 보관 기간(retention) 한계
- 저장 요구 때문에 클러스터가 불필요하게 커짐 → 비용 상승
- 파티션 크기가 클수록 **브로커 간 이동/재밸런싱 속도 저하**, 탄력성 저해

### 2. **Tiered Storage 구조**

- **`Local Tier`**: 기존 Kafka 저장소 (브로커의 로컬 디스크)
- **`Remote Tier`**: HDFS, S3 같은 원격 스토리지 (완료된 로그 세그먼트 저장)
- **Retention 정책 분리**:
    - Local: 짧게 (몇 시간 수준, 비용↑, 지연↓)
    - Remote: 길게 (며칠~몇 달, 비용↓, 지연↑)

### 3. **장점**

- **무한 확장 가능**: 저장소 용량을 CPU/메모리와 독립적으로 확장
- **비용 절감**: 로컬 디스크 부담 최소화
- **복구/재밸런싱 속도 향상**: 원격에 있는 데이터는 브로커에 꼭 복구할 필요 없음
- **데이터 파이프라인 단순화**: Kafka 자체가 장기 저장소 역할 가능 (따로 HDFS/S3로 복사 불필요)
- **읽기 성능 개선**:
    - 최신 데이터 읽기 (tail read) → 로컬 tier (빠름)
    - 과거 데이터 읽기 → Remote tier (네트워크 경유, 하지만 로컬 디스크 I/O와 격리돼서 안정적)

### 4. **성능 영향 (KIP-405 실험 결과)**

- 일반 고처리량 쓰기 워크로드: p99 지연 21ms → 25ms (소폭 증가)
- 과거 데이터 읽기:
    - 기존: 21ms → 60ms (지연 심함)
    - Tiered Storage: 25ms → 42ms (지연 있지만 격리 효과 덕분에 개선)

### 5. 현재 사용 가능 여부

| 플랫폼 구분 | 사용 가능? | 안정성 & 비고 |
| --- | --- | --- |
| Apache Kafka (버전 3.9 이상) | 사용 가능 (GA) | 안정적인 생산 환경 사용 가능 |
| Apache Kafka (3.6 ~ 3.8) | 사용 가능 (Early Access) | 프리뷰 용도; 버그 및 미완성 기능 존재 가능 |
| Confluent Platform | 사용 가능 | 완전 지원 및 설정 문서 제공 |
| Amazon MSK | 사용 가능 | 클라우드 기반 관리형 지원 |
| Aiven / Uber 등 | 사용 가능 | 자체 플러그인 및 검증된 사용 사례 존재 |

## 6.5.2 파티션 할당

### 1. 파티션과 복제본 할당 기본 원칙

- **균등 분배**: 모든 브로커에 파티션 복제본이 고르게 분배되도록 함
    
    (예: 브로커 6개, 파티션 10개, replication factor=3 → 총 30개 복제본 → 브로커당 5개씩)
    
- **중복 방지**: 같은 파티션의 복제본은 **서로 다른 브로커**에만 위치
- **랙 인식 (Rack Awareness)**: 가능하다면 **동일 랙에 같은 파티션 복제본이 배치되지 않도록** 하여 랙 단위 장애에도 **가용성을 확보**

### 2. 리더와 팔로워 할당 방식

- **리더 할당**: 무작위 브로커부터 시작해 round-robin 방식으로 각 파티션 리더를 배치
- **팔로워 할당**: 리더 위치 기준으로 일정 offset을 두고 다른 브로커에 배치
    
    (예: partition 0 리더가 broker 4라면, follower는 broker 5와 broker 0)
    

### 3. 랙 인식 적용 시

![image.png](attachment:adf6bfcd-7225-4e89-87d9-6cf03906e5d4:image.png)

- 브로커 목록을 **랙 교차 순서**로 배열
    - 예시: broker 0·1은 Rack 1, broker 2·3은 Rack 2 → [0,2,1,3] 순서
- 이렇게 하면 파티션 복제본이 항상 서로 다른 랙에 분산되어 **랙 전체 장애에도 데이터 손실 없이 가용성 보장**

### 4. 디렉터리 선택 규칙

- 각 브로커에 여러 디스크(디렉토리)가 있을 경우, **가장 적은 파티션을 가진 디렉토리**에 새 파티션을 할당
- 새 디스크 추가 시, 새로운 파티션은 자동으로 그 디스크에 몰림(언밸런스 → 점차 균형화)

### 5. 주의사항

- 파티션 할당은 **디스크 용량이나 부하**는 고려하지 않음
- 따라서 브로커마다 디스크 크기가 다르거나, 파티션 크기 차이가 큰 경우 **불균형** 발생 가능 → 운영자가 주의 깊게 관리해야 함

## 6.5.3 파일 관리

### **Retention & Segment 관리**

- Kafka는 데이터를 영구 보관하지 않고 **토픽별 보존 주기(retention)** 설정에 따라 삭제
    - 시간 기준 (예: 7일) 또는 용량 기준 (예: 1GB)
- **파티션 → 세그먼트 단위**로 나눠 저장 (기본: 1GB 또는 1주 중 작은 값)
- 현재 쓰는 세그먼트(Active Segment)는 **닫히기 전까지 삭제 불가**
    - 예: retention 1일이어도 active segment가 5일치라면 실제 보관은 5일
- 브로커는 모든 세그먼트(활성/비활성)에 대한 ****파일 핸들**을 열고 있음 → OS에서 많은 파일 핸들 필요

****파일 핸들**: 운영체제가 파일을 열었을 때 제공하는 일종의 참조 ID (디스크 파일을 가리키는 포인터)

- 프로세스가 파일을 일거나 쓰려면 반드시 해당 파일 핸들을 통해서만 접근 가능
- **카프카에서 파일 핸들을 열고 있다는 의미**:
    - 파티션 하나에 세그먼트가 10개라면 카프카 브로커 프로세스는 그 **10개 파일을 동시에 open 상태로 유지**
        - 현재 쓰는 세그먼트 → 계속 append 해야 하므로 열려 있어야 함
        - 과거 세그먼트 → 컨슈머가 과거 데이터를 읽을 수 있으므로 빠르게 접근할 수 있게 열어둠
- **파일핸들을 열고 있는 이유:**
    1. **성능 최적화 (지연 감소)**
        - 컨슈머가 과거 데이터를 읽으려 할 때마다 파일을 새로 열면 → OS 호출 비용(시스템 콜)이 매번 발생 → 레이턴시 증가
        - 미리 열어두면 곧바로 읽기 가능 → 지연 최소화
    2. **동시 접근 처리**
        - Kafka는 수천 개 이상의 파티션/세그먼트를 동시에 다룰 수 있음
        - 여러 컨슈머가 동시에 서로 다른 세그먼트를 읽을 수 있어야 함 → 항상 열려 있어야 빠른 동시 처리 가능
    3. **페이지 캐시 활용**
        - OS는 열린 파일의 일부를 ****메모리 캐시(page cache)**에 유지
        - 파일 핸들을 열어둬야 OS가 캐시를 유지하고 성능이 잘 나오게 됨

### 참고) 카프카에서 페이지 캐시가 중요한 이유

1. **빠른 읽기 성능**
    - Kafka는 컨슈머가 메시지를 읽을 때 디스크에서 직접 읽는 게 아니라 대부분 페이지 캐시에서 가져옴
    - 따라서 디스크 I/O 부하가 줄고 레이턴시가 낮아짐
2. **효율적인 쓰기 성능**
    - 프로듀서가 메시지를 브로커에 보내면 OS는 데이터를 디스크에 쓰기 전에 페이지 캐시에 먼저 저장
    - OS가 배치 단위로 디스크에 flush 하기 때문에 작은 쓰기도 효율적으로 처리 가능
3. **Zero-Copy 최적화**
    - Kafka는 네트워크로 메시지를 보낼 때, 애플리케이션이 직접 데이터를 복사하지 않고 OS의 페이지 캐시에서 바로 소켓으로 전송 가능
    - 이게 Kafka가 빠른 이유 중 하나

## 6.5.4 파일 형식

### **1. 파일 포맷**

- 세그먼트 파일에 저장된 메시지 포맷은 **프로듀서 ↔ 브로커 ↔ 컨슈머 간 전송 포맷과 동일**
- 덕분에 **zero-copy** 최적화 가능 (메시지를 재압축/복호화 없이 전달)

### **2. 메시지 배치(Message Batch)**

- Kafka 0.11(v2 포맷) 이후: **모든 메시지는 배치 단위로 전송**
    - 작은 지연(`linger.ms`)을 줘서 여러 메시지를 묶으면 **네트워크·디스크 효율 ↑, 압축 효율 ↑**
    - 파티션 단위로 배치 생성, 하나의 produce 요청에 여러 배치 포함 가능

### **3. 배치 헤더 정보**

- 메시지 포맷 버전(magic number)
- 배치의 첫 메시지 offset, 마지막 메시지 offset 차이
- 첫/마지막 메시지 timestamp
- 배치 크기(bytes)
- Leader epoch (리더 교체 시 복구용)
- 체크섬 (데이터 무결성 검증)
- 속성 비트 (압축, 타임스탬프 종류, 트랜잭션 여부 등)
- Producer ID, epoch, 시퀀스 번호 (Exactly-once 보장용)

### **4. 레코드(개별 메시지) 정보**

 배치 단위에 메타데이터를 모으고, 각 레코드에는 **차이 값만 기록** → **오버헤드 최소화**

- 레코드 크기
- Offset 차이 (배치 내 첫 메시지 기준)
- Timestamp 차이 (배치 내 첫 메시지 기준)
- **Payload**: 실제로 애플리케이션에서 전송하려는 데이터
    - **`Key` (옵션)**: 메시지 라우팅(파티션 결정)에 사용 가능
    - **`Value`**: 실제 데이터
    - **`Headers` (옵션)**: 추가 메타데이터, 여러 개의 key/value 쌍

### 5. **Control Batch**

- 데이터 배치 외에도 **트랜잭션 제어 배치** 존재
    - 예: 커밋(1), 중단(0) 표시
    - 컨슈머가 내부적으로 처리, 애플리케이션에는 전달되지 않음

### 6. **Message Format Down Conversion**

- v2 메시지가 저장되었지만 **구버전 컨슈머**가 읽으려 하면, 브로커가 **v2 → v1 변환** 수행
- 변환은 **CPU/메모리 소모↑** → 가능하면 피해야 함
- 모니터링 지표:
    - `FetchMessageConversionsPerSec`
    - `MessageConversionsTimeMs`
- 권장: **클라이언트 업그레이드**를 빨리 진행할 것

## 6.5.5 인덱스

- **오프셋 인덱스**: 메시지 오프셋 → 세그먼트 파일 내 위치로 매핑
- **타임스탬프 인덱스**: 타임스탬프 → 메시지 오프셋으로 매핑
    - Kafka Streams, 장애 복구 시 활용
- 인덱스도 세그먼트 단위로 관리 → 오래된 데이터 삭제 시 함께 제거됨
- 인덱스 손상 시 로그 세그먼트 파일을 다시 읽어 자동 복구 가능 → 안전

## 6.5.6 압착

- 일반 Retention 정책: 시간/용량 기반으로 오래된 메시지 삭제
- Compaction 정책: **키별 최신 값만 유지** (이전 값은 제거)
    - 예: 고객 주소, 애플리케이션 상태 저장
- Retention 정책과 **delete.and.compact**을 함께 적용 가능 → 최신 값도 retention 기간이 지나면 삭제

## 6.5.7 압착의 작동 원리

- 로그는 두 구역으로 나뉨
    - **Clean**: 이전에 이미 compact된 영역 (키당 최신 값만 있음)
    - **Dirty**: compaction 이후 새로 기록된 영역
        
        ![image.png](attachment:a2b6634d-ea9b-42d0-b1a3-d9a77b8a7d45:image.png)
        
- 브로커는 `log.cleaner.enabled` 옵션을 켜면 compaction 스레드 실행 → 각 스레드는 전체 파티션 크기 대비 더티 메시지의 비율이 가장 높은 파티션을 골라서 압착한 뒤 클린상태로 만듦
- 스레드는 dirty 영역을 읽고 **오프셋 맵(offset map)** 생성
    - 맵: `Key 해시(16B)` + `이전 메시지 오프셋(8B)` = 24B
    - 1GB 세그먼트(1KB 메시지 100만 개) → 약 24MB 맵 필요 → 메모리 효율적
- 맵을 기준으로 clean 영역 메시지를 검사
    - 키가 최신 맵에 없으면 → 유지
    - 키가 맵에 있으면 → 이미 더 최신 값이 있으므로 삭제
- 결과: **각 키당 최신 값만 남음**

![image.png](attachment:19558990-e345-47c1-8f89-095d9a304dc1:image.png)

## 6.5.8 삭제된 이벤트

- **완전 삭제 필요 시**: 애플리케이션은 `key`는 지정하고 `value=null` 메시지를 프로듀스 → 이를 **Tombstone 메시지**라고 함
- Compaction 시:
    - 해당 키의 기존 값들은 모두 제거
    - null 값(Tombstone)만 일정 기간 유지
- Tombstone 유지 이유:
    - 컨슈머가 이를 읽어 “삭제되었다”는 사실을 반영할 수 있도록 하기 위함
    - 예: RDBMS 동기화 시 Tombstone을 보고 DB에서 해당 사용자 삭제
- Tombstone 보관 기간 이후에는 Kafka 파티션에서도 완전히 제거됨

### DeleteRecords 메서드 (AdminClient)

- Kafka AdminClient는 `deleteRecords` 메서드 제공
- 특정 오프셋 이전의 모든 레코드 삭제 가능
- 내부적으로 **low-water mark**를 옮겨서 해당 오프셋 이전 데이터는 접근 불가 처리 후 정리됨
- retention 기반 토픽이나 compacted 토픽 모두 사용 가능
- Tombstone 방식과는 다른 메커니즘

## 6.5.9 토픽은 언제 압착되는가?

- Compaction은 **현재 활성 세그먼트**는 건드리지 않고 **비활성 세그먼트**만 처리
- 기본 동작: 토픽의 50%가 dirty records(압착 전 메시지)일 때 compaction 시작
    - 성능 저하 방지 vs 디스크 공간 낭비 최소화의 절충안
- 관리자 설정 옵션:
    - **`min.compaction.lag.ms`**: 메시지가 기록된 후 최소 얼마 동안은 압착하지 않음
    - **`max.compaction.lag.ms`**: 메시지가 기록된 후 최대 얼마 안에는 반드시 압착되도록 보장
        - 유럽 연합의 개인정보 보호법인 일반 데이터 보호 규정(GDPR) 요구사항(30일 내 특정 데이터 삭제)을 맞추는 데 활용
